{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install tensorflow==2.12.0\n",
        "!pip install efficientnet\n",
        "!pip install gradio opencv-python"
      ],
      "metadata": {
        "id": "FllpYXuufRNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "print(keras.__version__)\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "YFp4ocFqtvFF",
        "outputId": "f8164bff-a78e-4980-fa05-e6135fbc0a87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n",
            "2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwVOsP5uSa7N"
      },
      "source": [
        "# **GUI is created with Gradio**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "import gradio as gr\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "#I add this to solve problem\n",
        "import efficientnet.tfkeras\n",
        "\n",
        "# Load the segmentation model\n",
        "segmentation_model = load_model('/content/drive/MyDrive/Colab Notebooks/Project Implementation/New_Experiments/Saved_model/Segmentation_UNET_Resnet34_ORIGA&RIMONEDL_with_Aug_V2_2040_without_denoising_Epoch_50_batch_16.hdf5', compile=False)\n",
        "\n",
        "# Load the classification model\n",
        "\n",
        "classification_model = load_model('/content/drive/MyDrive/Colab Notebooks/Project Implementation/Classification/Saved_model_HRF/Classification_EfficientNetB0_Aug_Train_HRF_epoch_15_batch_4_with_two_droupout_lr=0.0001_model.h5')\n",
        "\n",
        "# Define the preprocessing function for the segmentation model\n",
        "def preprocess_segmentation_image(image):\n",
        "    # Apply resize\n",
        "    new_width = 128\n",
        "    new_height = 128\n",
        "    dim = (new_width, new_height)\n",
        "    resized_image = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)\n",
        "    # Preprocess the image\n",
        "    preprocessed_image = resized_image / 255.\n",
        "    return preprocessed_image\n",
        "\n",
        "# Define the preprocessing function for the classification model\n",
        "def preprocess_classification_image(image):\n",
        "    # Resize the image\n",
        "    img = cv2.resize(image, (224, 224))\n",
        "    # Normalize the image\n",
        "    img = img.astype('float32') / 255\n",
        "    # Reshape the image to match the model's input shape\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "def predict(image):\n",
        "    # Convert the image from RGB to BGR\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Preprocess the image for segmentation\n",
        "    preprocessed_image = preprocess_segmentation_image(image)\n",
        "\n",
        "    # Make segmentation prediction\n",
        "    segmentation_prediction = segmentation_model.predict(np.expand_dims(preprocessed_image, axis=0))\n",
        "    segmentation_mask = np.argmax(segmentation_prediction, axis=3)[0]\n",
        "\n",
        "    # Resize the mask to match the image size\n",
        "    segmentation_mask_resized = cv2.resize(segmentation_mask.astype(np.uint8), (image.shape[1], image.shape[0]))\n",
        "\n",
        "    # Apply the mask to the original image\n",
        "    segmented_image = cv2.bitwise_and(image, image, mask=segmentation_mask_resized)\n",
        "\n",
        "    # Convert the segmented image to grayscale\n",
        "    gray = cv2.cvtColor(segmented_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Find contours in the grayscale image\n",
        "    contours, _ = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Find the bounding rectangle for each contour\n",
        "    bounding_rectangles = [cv2.boundingRect(contour) for contour in contours]\n",
        "\n",
        "    # Get the x, y, w, h of the bounding rectangle with the maximum area\n",
        "    x, y, w, h = max(bounding_rectangles, key=lambda rect: rect[2] * rect[3])\n",
        "\n",
        "    # Crop the black area from the segmented image\n",
        "    cropped_segmented_image = segmented_image[y:y+h, x:x+w]\n",
        "\n",
        "    # Preprocess the cropped segmented image for classification\n",
        "    preprocessed_segmented_image = preprocess_classification_image(cropped_segmented_image)\n",
        "\n",
        "    # Make classification prediction\n",
        "    classification_prediction = classification_model.predict(preprocessed_segmented_image)\n",
        "    label = \"Glaucoma\" if classification_prediction[0][0] >=   0.213948 else \"Normal\"  #I change from 0.5 to  0.213948\n",
        "\n",
        "    # Convert the cropped segmented image back to RGB\n",
        "    cropped_segmented_image = cv2.cvtColor(cropped_segmented_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    return cropped_segmented_image, label\n",
        "\n",
        "iface = gr.Interface(fn=predict, inputs=\"image\", outputs=[\"image\", \"text\"], title=\"Glaucoma Detection on Enhanced Retinal Images Based Deep Learning\")\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "e3d048fd-6f62-423f-c2a8-39168b8a1ca6",
        "id": "HGl4vaMGUJda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://0177a09adaaefc466e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0177a09adaaefc466e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}